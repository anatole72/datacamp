{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Science Track: Course 17**\n",
    "# **Machine Learning with the Experts: School Budgets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 1: Exploring the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Hair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jamal</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Curly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luisa</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Straight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jenny</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Wavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Straight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Eyes      Hair\n",
       "Jamal  Brown     Curly\n",
       "Luisa  Brown  Straight\n",
       "Jenny   Blue      Wavy\n",
       "Max     Blue  Straight"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A column for each possible value\n",
    "\n",
    "face_df = pd.DataFrame({'Eyes':['Brown','Brown','Blue','Blue'],\n",
    "                   'Hair':['Curly','Straight','Wavy','Straight']},\n",
    "                  index=['Jamal','Luisa','Jenny','Max'])\n",
    "face_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eyes_Blue</th>\n",
       "      <th>Eyes_Brown</th>\n",
       "      <th>Hair_Curly</th>\n",
       "      <th>Hair_Straight</th>\n",
       "      <th>Hair_Wavy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Jamal</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Luisa</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jenny</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Eyes_Blue  Eyes_Brown  Hair_Curly  Hair_Straight  Hair_Wavy\n",
       "Jamal          0           1           1              0          0\n",
       "Luisa          0           1           0              1          0\n",
       "Jenny          1           0           0              0          1\n",
       "Max            1           0           0              1          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_df_dummies = pd.get_dummies(face_df)\n",
    "face_df_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric</th>\n",
       "      <th>text</th>\n",
       "      <th>with_missing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.856306</td>\n",
       "      <td></td>\n",
       "      <td>4.433240</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.973454</td>\n",
       "      <td>foo</td>\n",
       "      <td>4.310229</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.829785</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>2.469828</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-15.062947</td>\n",
       "      <td></td>\n",
       "      <td>2.852981</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.786003</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>1.826475</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     numeric     text  with_missing label\n",
       "0 -10.856306               4.433240     b\n",
       "1   9.973454      foo      4.310229     b\n",
       "2   2.829785  foo bar      2.469828     a\n",
       "3 -15.062947               2.852981     b\n",
       "4  -5.786003  foo bar      1.826475     a"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preview the data\n",
    "sample_df = pd.read_csv('datasets/sample_data.csv',index_col=0)\n",
    "sample_df.text.fillna('',inplace=True)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      "numeric         1000 non-null float64\n",
      "text            1000 non-null object\n",
      "with_missing    822 non-null float64\n",
      "label           1000 non-null object\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 39.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Summarize the data\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric</th>\n",
       "      <th>with_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>822.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.395641</td>\n",
       "      <td>3.025194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.012883</td>\n",
       "      <td>0.994960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-32.310550</td>\n",
       "      <td>-0.801378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.845565</td>\n",
       "      <td>2.386520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.411856</td>\n",
       "      <td>3.022887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.688658</td>\n",
       "      <td>3.693381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35.715792</td>\n",
       "      <td>5.850708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           numeric  with_missing\n",
       "count  1000.000000    822.000000\n",
       "mean     -0.395641      3.025194\n",
       "std      10.012883      0.994960\n",
       "min     -32.310550     -0.801378\n",
       "25%      -6.845565      2.386520\n",
       "50%      -0.411856      3.022887\n",
       "75%       6.688658      3.693381\n",
       "max      35.715792      5.850708"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400277 entries, 134338 to 415831\n",
      "Data columns (total 25 columns):\n",
      "Function                  400277 non-null object\n",
      "Use                       400277 non-null object\n",
      "Sharing                   400277 non-null object\n",
      "Reporting                 400277 non-null object\n",
      "Student_Type              400277 non-null object\n",
      "Position_Type             400277 non-null object\n",
      "Object_Type               400277 non-null object\n",
      "Pre_K                     400277 non-null object\n",
      "Operating_Status          400277 non-null object\n",
      "Object_Description        375493 non-null object\n",
      "Text_2                    88217 non-null object\n",
      "SubFund_Description       306855 non-null object\n",
      "Job_Title_Description     292743 non-null object\n",
      "Text_3                    109152 non-null object\n",
      "Text_4                    53746 non-null object\n",
      "Sub_Object_Description    91603 non-null object\n",
      "Location_Description      162054 non-null object\n",
      "FTE                       126071 non-null float64\n",
      "Function_Description      342195 non-null object\n",
      "Facility_or_Department    53886 non-null object\n",
      "Position_Extra            264764 non-null object\n",
      "Total                     395722 non-null float64\n",
      "Program_Description       304660 non-null object\n",
      "Fund_Description          202877 non-null object\n",
      "Text_1                    292285 non-null object\n",
      "dtypes: float64(2), object(23)\n",
      "memory usage: 79.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('datasets/TrainingData.csv',index_col=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 FTE         Total\n",
      "count  126071.000000  3.957220e+05\n",
      "mean        0.426794  1.310586e+04\n",
      "std         0.573576  3.682254e+05\n",
      "min        -0.087551 -8.746631e+07\n",
      "25%         0.000792  7.379770e+01\n",
      "50%         0.130927  4.612300e+02\n",
      "75%         1.000000  3.652662e+03\n",
      "max        46.800000  1.297000e+08\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAErCAYAAADue+XJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3df1xUVf4/8Ncww+9RhBnQAH8BsqKJoEioCSi0bemaum2ubj4SNQ20Qt02M9PaTZd2UxHBTFH8VW6uCZVp9mCRH4n0AREU8BegJQEiDCIIiDDn+4fLfEHARmRmFF7Px4PHQ87ce+77DsWLc++dcyRCCAEiIiI9MTJ0AURE1LMweIiISK8YPEREpFcMHiIi0isGDxER6RWDh4iI9IrBQ3rz/vvvw8XFRSd9JyYmQiKRoKioqN3vu9quXbsgk8l00ndnXL16FQEBAbC0tIREIuny/leuXIm+fftCIpFg165dWu1z73v0MD8TXf88Sb8YPPRQ5s6dC4lEAolEAplMBhsbG4wdOxYffPABVCpVq23/8pe/IC0tTeu+XVxc8P7772u17bhx41BSUgJ7e/sHKf9XFRUVQSKRIDExsVX7zJkz8csvv3TpsR7GunXrUFZWhqysLJSUlLS7zU8//YSJEydCLpfDy8sL2dnZrV7fvHkznnvuuTb7/fjjj/jHP/6Bbdu2oaSkBDNnztTJOTSTyWRtwk1XP18yDAYPPbQJEyagpKQEP//8M1JSUvDqq69i//79GD58OC5evKjZTi6XQ6lUdvnxGxoaYGJign79+sHISD//SZubm6Nv3756OZY2Ll26BG9vbwwZMgT9+vVrd5vly5dDqVQiKysLY8eOxYIFCzSvXblyBWFhYdi2bVu7fRsZGeGFF15Av379YG5urrPz6Ii+f76kY4LoIbzyyisiICCgTXtVVZVwcnISEydO1LStWbNGODs7a76/evWqmDFjhlAoFMLMzEwMHjxY/POf/xRCCOHn5ycAtPq6fPmyOH78uAAgDh8+LMaPHy9MTU3F5s2bNe1Xr14VQgjN919//bUYM2aMMDU1FcOGDRPff/+95vj37tNMKpWKmJgYIYRoU8PAgQOFEELExMQIqVTaar9vv/1WjBo1SpiYmAhbW1sRHBwsampq2rxXn376qRgwYIDo1auXmDp1qigrK7vve3zz5k2xcOFCoVQqhampqRg9erQ4duyY5vV7a3zllVfa7WfYsGHi6NGjQggh8vLyhIWFhea1wMBA8emnn7bZ55VXXmnTf8tzaWnv3r2i5a+Ue9+jjt7vlgYOHNju8Tr6+X777bfCx8dHmJmZiVGjRomcnByRk5Mjxo8fL8zNzcWYMWNEbm5uq2NkZGSIZ555RlhaWgqlUimmT58urly50mFN1PX45wPpRO/evREcHIzExERcv3693W1CQkJQVVWF+Ph4nDt3Djt27ICjoyMA4NChQxg0aBCWL1+OkpISlJSUoH///pp9ly9fjr/+9a84d+4cpk2b1mEdy5Ytw+rVq3H69Gn4+Phg6tSpD3SJLDMzEwDw5ZdfoqSkBOnp6e1ud+bMGUydOhW+vr7IysrC7t27cfjwYbz22muttktPT8fx48fx7bff4rvvvkNWVhb+8pe/3LeGefPm4dixY9i3bx9Onz6N8ePHY8qUKTh//jwAoKSkBGPHjsXs2bNRUlKCTZs2tdvPyJEj8d1330GtVuPIkSPw9PQEAGzfvh1qtRqvvvpqm302bdqE8PBwSKVSzc9Bl9LT0yGVShEeHq7V8d59912sXbsWp06dgomJCWbNmoXg4GB88MEHmragoCDN9nl5efDz88PYsWORkZGBhIQESKVSPPPMM6ivr9fpuVELhk4+erx1NOIRQoijR48KAOLHH38UQrQd8bi7u4s1a9Z02Lezs3Ob15v/0t2zZ0+77ff+RRwdHa3Z5s6dO2LAgAHi3XffbXefZi1HPFevXhUAxPHjx1ttc+9f8y+//LIYM2ZMq23i4uKERCLR/DX9yiuvCKVSKerr6zXb/OMf/xD9+vXr8D24dOmS5i/7ljw9PUVQUJDmez8/PzF//vwO+xFCiF9++UVMnTpV9O/fXwQEBIgLFy6IoqIiYW9vLwoLC8XatWuFq6urGDNmjEhLS+vwXJvPRRcjHiFav/8d7dv8fWxsrGabAwcOCADi4MGDmrZDhw4JAKK6ulpT98yZM1v1XV9fL8zNzVv1Rbr16DyWQ92O+N/8sx09ZRUaGopFixbh6NGj8Pf3x+TJk+Hr66tV397e3lptN3bsWM2/ZTIZvL29kZeXp9W+DyI3NxeTJk1q1ebn5wchBPLy8jBw4EAAgJubG0xNTTXbODg44Nq1ax3221zrve+Lr68vTp48+UA12tvb46uvvmrVNmXKFKxYsQLnzp3Dnj178OOPP+LEiRN48cUXUVBQABMTkwc6xoN47rnnkJKSovm+pqbmgfsYOXKk5t/N97bc3d3btJWVlUEulyM9PR35+fmQy+Wt+qmvr8elS5ce+PjUOQwe0pmcnBxIJBI4OTm1+3pQUBB+97vf4bvvvsPx48fx3HPPYfr06di3b9+v9m1padmpmkSLydibb1S3bGtqaoJare5U3x0FbMv2e3+RSySSVsfXlhDioR+b3rt3L6qqqrBkyRIsX74cv//979GnTx9MnjwZdXV1uHDhAkaMGNHuvkZGRm3qvnPnzgMdPzo6GnV1dZ2uHwCMjY01/25+P9pra/6ZqtVqzJkzBytWrGjTl0KheKhaSHu8x0M6cfPmTXzyyScICAi47//QTzzxBIKCgrBnzx7s2LEDn332GW7evAng7i/ppqamh6qj5ePbjY2NSE9Ph5ubGwDAzs4OAFBcXKzZJisrq9Uv1Oag+LU6hg8fjqSkpFZtSUlJkEgkGDZsWKfrHz58OAAgOTm5VXtKSormtc64du0a3nnnHezYsQMSiQRqtVoTHEIINDY23jeA7ezsWr1vwP+/H6YtBwcHuLi4aL6adcXPvSNeXl44c+YMnJ2dWx3bxcUF1tbWOjkmtcXgoYfW0NCA0tJSlJSUIC8vDzt37oS3tzdu376NTz75pMP9lixZgiNHjqCgoAC5ubk4dOgQ+vfvj169egEABg8ejBMnTuDnn39GeXl5p0YiYWFhOHLkCM6dO4fg4GBcu3YNwcHBAO5+TmjgwIF4//33cf78efzwww9YunRpq5GEUqmEXC7H999/j9LSUlRWVrZ7nLfeeguZmZlYtmwZzp8/j++++w6vv/46/vznP2PAgAEPXHczZ2dn/PGPf0RISAiOHTuG8+fP480330ROTg7eeuutTve7ePFihIaGwtXVFcDdS3f/+c9/cOrUKURERMDMzAy/+c1vOtw/MDAQ58+fR2RkJAoKCrB9+3YcOHCg0/W0NHjwYBw/fhzFxcUoLy/vkj6brVy5EufOncPLL7+M//u//8Ply5dx/PhxvPnmmygsLOzSY1HHGDz00FJSUvDEE0+gf//+GD9+PLZt24bZs2cjJyfnvjMVCCEQGhqKJ598Er6+vrh16xaOHj2q+cX/wQcfoKqqCr/5zW9ga2uLn3/++YFr+/jjj/Hee+/Bw8MDJ06cwFdffaV5ck4mk+GLL75AWVkZPD09sXjxYqxdu7bVZ0WMjIwQFRWFAwcOoH///ponwe7l7u6Or7/+GklJSRg5ciTmzJmDyZMnY+vWrQ9c872io6Px7LPP4uWXX8bIkSNx4sQJHD58GEOHDu1Uf19++SWuXr2KpUuXatqmT5+OP/3pT3jmmWcQGRmJzz//HGZmZh32ERgYiA8//BD/+Mc/MHLkSCQkJGD16tWdqude69evx6lTpzB48GDY2tp2SZ/N3NzckJqaipqaGjz77LMYNmwYXn31VdTV1aFPnz5deizqmER05gIzERFRJ3HEQ0REesXgISIivWLwEBGRXjF4iIhIrxg8RF2I68a0j+8LtcTgISIivWLwEJFONTQ0GLoEesQweKjbuXbtGubOnQtbW1v06tUL48ePbzXlTPNlnyNHjmDs2LEwNzfH6NGjkZubi9zcXDz99NOwsLBoM6Fo81LO8fHxGD58OMzMzODt7f2rU8WkpaXB19cX5ubmsLa2xuzZs1FWVgYAKCwshJGREVJTU1vtk5SUBCMjI82n6WtqavDmm2/CwcEBFhYW8PT0xKFDhx7ovO+Vn58PiUSC/Px8TdvAgQM1H7AF7i4QJ5FINO9DdXU1Fi1aBFtbW5iZmcHLywvff/99m+0/++wzPP/887C0tMTKlSvbHFutVmPJkiVwdHRETk4OAOCrr76Cp6cnLCws0KdPH3h7e+P06dP3fW/pMWWYSbGJdKO2tla4ubmJGTNmiPT0dHHp0iXx4YcfChMTE5GXlyeE+P9T6nt4eIj//ve/Ijc3V/j4+IgRI0aICRMmiPj4eJGXlyfGjx8vvL29NX3HxMQIiUQiPD09RWJiosjOzhaTJ08W/fr1E7du3WrVd/P0/SUlJaJXr15i1qxZ4syZMyIlJUWMGDFCPP3005p+f/vb34q5c+e2Oo+XX35ZBAYGCiGEUKvVwt/fX/j5+YmUlBRRUFAgPv30U2FsbCzi4+O1Pu/2DBgwQGzdulUIIUR+fr4wMzMTcrlcnD9/XgghRHR0dKtlG1588UUxcOBA8d1334m8vDzxxhtvCGNjY3Hu3DkhhBCXL18WAISDg4PYu3evKCgoEIWFha3el7q6OjFjxgzh5uYmfvrpJ837ZGxsLD766CNRWFgo8vLyxGeffSbOnDnzoP8J0GOAwUPdSkxMjHBwcBB37txp1T5x4kTx5ptvCiE6v5ZLTEyMAKD5ZS+EECqVSlhaWort27e36rs5eFatWiUcHBzE7du3NftkZWUJACIpKUkIIcSXX34pLCwsxI0bN4QQQlRWVgpzc3Nx4MABTZ+mpqaa15sFBQWJF154Qevzbs8rr7wi/vjHPwohhNi2bZuYNGmSeO6550RUVJQQQojZs2eLWbNmCSG0WxuoOXj+9re/tdqm+X05e/asmDBhghg3bpyoqKjQvJ6ZmalZZZa6P15qo24lPT0dpaWl6NOnD+RyueYrJSWlzXorD7KWS0st1/ixtraGm5tbh2v85ObmwsfHp9VyCCNHjoSVlRVyc3MBAFOnToWVlRU+//xzAMC+ffsgl8vxwgsvaM6poaEBDg4Orc5p3759mnN6kPNuadKkSTh+/DiEEEhISEBAQAAmTpyIhIQEAMDx48c16wzdb22g5nNp1tF6Sc8//zwAID4+HjY2Npp2d3d3PPvss3jyyScxffp0bNq0CVevXu2wbnq8cT0e6lbUajXc3NwQGxvb5jULC4tW3z/oWi4dEb8y3eGvrdMjk8kwf/58bN++HcHBwYiOjsbcuXM1YaVWq2FlZdXuststt9H2vFsKCAhAeXk5zpw5o5ml2djYGGFhYcjNzUVJSUmbBe7uJdpZG6ij9ZKmTp2KmJgYnDx5slW/UqkUR48eRXp6OuLj4/Hll19ixYoV+M9//oMpU6bc9/j0+OGIh7oVLy8vFBYWonfv3m3WW7G3t++SY7Rc4+fGjRs4f/68Zo2few0fPhwnT55s9WRXdnY2qqqqWq2n8+qrryI7Oxtbt25FdnY2FixY0Oqcbty4gfr6+jbn1LzkQmfP28HBAUOGDMHmzZtRV1cHLy8veHp6QgiBjRs3YuDAgZqF/LpibaAVK1bggw8+wJQpU1o9lADcDWJvb2+sXLkSycnJ8PPzQ0xMjFb90mPGsFf6iLpWXV2dGD58uPDy8hLHjh0Tly9fFmlpaWLdunWaezr33ocRQoiUlJQ29xhOnjwpAIhLly4JIf7/wwWjR48WSUlJ4syZM+L3v/+9sLOzEzU1Ne32XVpaqnm44OzZs+0+XNDs+eefFyYmJsLf379Vu1qtFoGBgWLIkCHi0KFDoqCgQGRkZIiIiAixbds2rc+7I4sWLRIymUxMmTJF0zZt2jQhk8k0926a/fGPf9Q8XHDu3LkOHy5ISUlptd+978umTZuEmZmZOHz4sBBCiBMnToi//e1vIi0tTfz0008iPj5ePPHEE2LVqlX3rZ0eTwwe6nbKy8vFa6+9Juzt7YWxsbGwt7cX06ZNE5mZmUKIhwseqVQqjh07JoYOHSpMTEyEl5eXSE9P1+zTXt8nT54UEyZMEGZmZsLKykrMmjVLXLt2rU3dcXFxAoD4/PPP27xWW1sr3n77bTFo0CBhbGws+vbtK5599lnx3//+V+vz7kjzgxUbNmzQtEVERAgAYu/eva22raqqEgsXLhRKpVKYmJiI0aNHi2PHjmle1zZ4hBBiy5YtwtTUVMTFxYmcnBzx3HPPib59+woTExMxYMAA8Ze//KXVQxnUfXA9HiIt7dq1CwsWLEBjY6NO+t+yZQtWr16NX375Baampjo5BtGjgA8XEBlYTU0N8vPz8fHHH2PJkiUMHer2+HABkYEtWbIE3t7ecHNzw9tvv23ocoh0jpfaiIhIrzjiISIivWLwEBGRXjF4iIhIr/hUWweKi4sNXYLOKJVKlJeXG7oMnenO59edzw3g+T3utJ0dhCMeIiLSKwYPERHpFYOHiIj0isFDRER6xeAhIiK9YvAQEZFeMXiIiEivGDxERKRXDB4iItIrzlzQgaZXpxq6BJ25ZugCdKw7n193Pjfg0Tk/6favDV1Ct8YRDxER6RWDh4iI9IrBQ0REesXgISIivWLwEBGRXunlqbYtW7YgMzMTVlZWWL9+PQBg7969OHXqFGQyGfr27YuQkBBYWloCAGJjY5GQkAAjIyMEBQXBw8MDAJCVlYWYmBio1WoEBARg2rRpAICysjKEh4ejpqYGgwcPxuuvvw6ZTIY7d+4gMjIShYWF6NWrF0JDQ2FnZ6ePUyYiog7oZcTj7++PlStXtmpzd3fH+vXr8fHHH+OJJ55AbGwsAKCoqAipqanYsGED3n33XezYsQNqtRpqtRo7duzAypUrsXHjRpw4cQJFRUUAgH379mHy5MmIiIiApaUlEhISAAAJCQmwtLTE5s2bMXnyZHz22Wf6OF0iIroPvQTPsGHDIJfLW7WNHDkSUqkUAODq6gqVSgUASE9Px7hx42BsbAw7Ozv069cP+fn5yM/PR79+/dC3b1/IZDKMGzcO6enpEEIgNzcXPj4+AO6GXHp6OgAgIyMD/v7+AAAfHx/k5ORACKGPUyYiog48Evd4EhISNJfTVCoVFAqF5jUbGxuoVKo27QqFAiqVCtXV1bCwsNCEWPP29/YllUphYWGB6upqfZ0WERG1w+AzFxw6dAhSqRQTJkwAgA5HJO21SySS+/b9IPvEx8cjPj4eABAWFnbffomoe1MqlTrpVyaT6azvx4lBgycxMRGnTp3C6tWrNYGgUChQUVGh2UalUsHGxgYAWrVXVFTA2toavXr1Qm1tLZqamiCVSltt39yXQqFAU1MTamtr21zyaxYYGIjAwEBdnSoRPUbKy8t10q9SqdRZ348Ce3t7rbYz2KW2rKwsfPXVV3j77bdhamqqaffy8kJqairu3LmDsrIylJSUwMXFBc7OzigpKUFZWRkaGxuRmpoKLy8vSCQSDB8+HGlpaQDuhpmXlxcAYPTo0UhMTAQApKWlYfjw4b86SiIiIt2SCD3cbQ8PD0deXh6qq6thZWWFl156CbGxsWhsbNSMQIYMGYKFCxcCuHv57fjx4zAyMsLcuXPh6ekJAMjMzMTu3buhVqsxceJEzJgxAwBw7dq1No9TGxsbo6GhAZGRkbh8+TLkcjlCQ0PRt29frWq+OtlLB+8EET0OdDVJKEc8d+kleB5HDB6inovB0zmP/KU2IiLqmRg8RESkVwweIiLSKwYPERHpFYOHiIj0isFDRER6xeAhIiK9YvAQEZFeMXiIiEivGDxERKRXDB4iItIrBg8REekVg4eIiPSKwUNERHrF4CEiIr1i8BARkV4xeIiISK8YPEREpFcMHiIi0isGDxER6RWDh4iI9IrBQ0REesXgISIivWLwEBGRXsn0cZAtW7YgMzMTVlZWWL9+PQCgpqYGGzduxPXr12Fra4ulS5dCLpdDCIGYmBicPn0apqamCAkJgZOTEwAgMTERhw4dAgDMmDED/v7+AIDCwkJERUWhoaEBnp6eCAoKgkQi6fAYRERkOHoZ8fj7+2PlypWt2uLi4jBixAhERERgxIgRiIuLAwCcPn0apaWliIiIwMKFCxEdHQ3gblAdPHgQ69atw7p163Dw4EHU1NQAALZv345FixYhIiICpaWlyMrKuu8xiIjIcPQSPMOGDWsz0khPT4efnx8AwM/PD+np6QCAjIwM+Pr6QiKRwNXVFbdu3UJlZSWysrLg7u4OuVwOuVwOd3d3ZGVlobKyEnV1dXB1dYVEIoGvr6+mr46OQUREhqOXS23tqaqqgrW1NQDA2toaN2/eBACoVCoolUrNdgqFAiqVCiqVCgqFQtNuY2PTbnvz9vc7Rnvi4+MRHx8PAAgLC+uisySix1HL30FdSSaT6azvx4nBgqcjQog2bRKJpN1tJRJJu9t3RmBgIAIDA7ukLyJ6vJWXl+ukX6VSqbO+HwX29vZabWewp9qsrKxQWVkJAKisrETv3r0B3B2xtPzBVFRUwNraGjY2NqioqNC0q1QqWFtbQ6FQtGqvqKiAjY3NfY9BRESGY7Dg8fLyQlJSEgAgKSkJY8aM0bQnJydDCIGLFy/CwsIC1tbW8PDwQHZ2NmpqalBTU4Ps7Gx4eHjA2toa5ubmuHjxIoQQSE5OhpeX132PQUREhiMRXXWt6j7Cw8ORl5eH6upqWFlZ4aWXXsKYMWOwceNGlJeXQ6lUYtmyZZrHqXfs2IHs7GyYmJggJCQEzs7OAICEhATExsYCuPs49cSJEwEABQUF2LJlCxoaGuDh4YF58+ZBIpGgurq63WNo4+pkL928GUT0yJNu/1on/fJS2116CZ7HEYOHqOdi8HTOI3+Ph4iIeiYGDxER6RWDh4iI9Eqrz/H88MMPGDRoEBwdHVFcXIxPP/0URkZGWLBgARwcHHRdIxERdSNajXi++OILzdNge/bsgbOzM9zc3DTzqBEREWlLq+C5efMm+vTpg4aGBly4cAGzZs3Ciy++iCtXrui4PCIi6m60utTWu3dvlJaW4ueff4azszOMjY1x+/ZtXddGRETdkFbB84c//AFvv/02jIyMsHTpUgDA2bNnMXDgQJ0WR0RE3Y/WHyBtHuGYmpoCuDvzsxACffr00V11BsQPkBL1XPwAaed0+QdIGxoa8OOPP+Krr74CADQ1NaGpqalz1RERUY+lVfDk5eUhNDQUKSkp+PLLLwEApaWl2L59u06LIyKi7ker4Nm1axdCQ0Px7rvvQiqVAgBcXFxQUFCg0+KIiKj70Sp4rl+/jhEjRrRqk8lkvNRGREQPTKvgcXR0RFZWVqu2s2fPYsCAATopioiIui+tHqeeM2cOPvroI3h6eqKhoQHbtm3DqVOn8NZbb+m6PiIi6ma0fpxapVIhJSUF169fh1KpxIQJE6BQKHRdn8HwcWqinouPU3eOto9TazXiAQAbGxv8/ve/R1VVFaytrTtdGBER9WxaBc+tW7cQHR2NtLQ0yGQy7N27FxkZGcjPz8ef/vQnXddIRETdiFYPF2zfvh0WFhbYsmULZLK7WeXq6orU1FSdFkdERN2PViOes2fP4tNPP9WEDnB34tCqqiqdFUZERN2TViMeCwsLVFdXt2orLy/nvR4iInpgWgVPQEAA1q9fj5ycHAghcPHiRURFReGZZ57RdX1ERNTNaHWp7YUXXoCxsTF27NiBpqYmfPLJJwgMDMTzzz+v6/qIiKib0Sp4JBIJJk+ejMmTJ3d5AYcPH0ZCQgIkEgn69++PkJAQ3LhxA+Hh4aipqcHgwYPx+uuvQyaT4c6dO4iMjERhYSF69eqF0NBQ2NnZAQBiY2ORkJAAIyMjBAUFwcPDAwCQlZWFmJgYqNVqBAQEYNq0aV1+DkREpD2tLrX99a9/xbffftvlDxOoVCocPXoUYWFhWL9+PdRqNVJTU7Fv3z5MnjwZERERsLS0REJCAgAgISEBlpaW2Lx5MyZPnozPPvsMAFBUVITU1FRs2LAB7777Lnbs2AG1Wg21Wo0dO3Zg5cqV2LhxI06cOIGioqIuPQciInowWgXPjBkzcO7cOSxZsgTr1q3DDz/8gIaGhi4pQK1Wo6GhAU1NTWhoaECfPn2Qm5sLHx8fAIC/vz/S09MBABkZGfD39wcA+Pj4aO45paenY9y4cTA2NoadnR369euH/Px85Ofno1+/fujbty9kMhnGjRun6YuIiAxDq0ttPj4+8PHxQU1NDVJTU3Hs2DFER0fD29sbvr6+ePLJJzt18ObZEIKDg2FiYoKRI0fCyckJFhYWmuUXbGxsoFKpANwdITVP0yOVSjVP26lUKgwZMqRVv837tJzWR6FQ4NKlS52qlYiIuobWU+YAgFwuh5+fH8zMzPD111/jxx9/xLlz52BkZIT58+fD3d39gQ5eU1OD9PR0REVFwcLCAhs2bGgzC3ZL7U0rJ5FI2m2/3/btiY+PR3x8PAAgLCxMm/KJqJtSKpU66Vcmk+ms78eJVsGjVqtx5swZJCcnIzMzE66urpg2bRq8vb1hYmKCtLQ0bN68+YFXJD179izs7OzQu3dvAMBTTz2FCxcuoLa2Fk1NTZBKpVCpVLCxsQFwd8RSUVEBhUKBpqYm1NbWQi6Xa9qbtdynZXtFRUWHnz0KDAxEYGDgA9VPRN2Triby5CShd2l1j2fRokXYu3cvBg4ciA0bNmDlypV4+umnYWJiAuDupThHR8cHLlKpVOLSpUu4ffs2hBA4e/YsHB0dMXz4cKSlpQEAEhMT4eV1d6bo0aNHIzExEQCQlpaG4cOHQyKRwMvLC6mpqbhz5w7KyspQUlICFxcXODs7o6SkBGVlZWhsbERqaqqmLyIiMgytlkUoKCiAs7OzTgo4cOAAUlNTIZVKMWjQILz22mtQqVRtHqc2NjZGQ0MDIiMjcfnyZcjlcoSGhqJv374AgEOHDuH48eMwMjLC3Llz4enpCQDIzMzE7t27oVarMXHiRMyYMUOrurgsAlHPxWUROjmo1NMAABl/SURBVEfbEY/W6/GUlJTgxIkTmstY48aN0/ogjyMGD1HPxeDpnC691JaRkYEVK1bgl19+gVwuR3FxMd555x1kZGQ8VJFERNTzaPVwwf79+/HWW2+1emw6NzcXO3fu5D0TIiJ6IFqNeFQqFdzc3Fq1DR06tNUTY0RERNrQKngGDRqEb775plXb4cOHMWjQIF3URERE3ZhWl9oWLFiAjz76CEePHtV8ZsbU1BR//etfdV0fERF1M1oFj4ODAzZu3IiLFy+isrISNjY2cHFxabUiKRERkTa0Tg6pVNrmPg8REdGD6jB4goODtergk08+6bJiiIio++sweF5//XV91kFERD1Eh8EzbNgwfdZBREQ9hFb3eBobG/Hll1/ixIkTqKyshLW1NcaNG4cZM2ZoJgolIiLShlbBs337dhQXFyMoKAi2tra4fv064uLiEB0djZCQEF3XSERE3YhWwZOeno7NmzfD0tISAODo6IghQ4bwPhARET0wrWYu6NOnD27fvt2qraGhocNF1YiIiDqi1YjH19cX69atw+9+9zvNzAXHjh2Dr68vcnJyNNu1nESUiIioPVqtx7N48eJf70giQWRkZJcU9SjgejxEPRfX4+kcbdfj0WrEExUV9VDFEBERNdPqHg8REVFX0WrEc+XKFezevRtXrlxBfX19q9f279+vk8KIiKh70ip4Nm3ahKeeegpBQUH8wCgRET0UrYLnxo0bmDlzJiQSia7rISKibk6rezx+fn744YcfdF0LERH1AFqNeKZNm4ZVq1YhNjYWVlZWrV5bs2aNTgojIqLuSavg2bBhA+zs7ODt7c17PERE9FC0fqpt586dOlnq+tatW9i6dSuuXr0KiUSC4OBg2NvbY+PGjbh+/TpsbW2xdOlSyOVyCCEQExOD06dPw9TUFCEhIXBycgIAJCYm4tChQwCAGTNmwN/fHwBQWFiIqKgoNDQ0wNPTE0FBQbxXRURkQFrd43Fzc0NRUZFOCoiJiYGHhwfCw8Pxr3/9Cw4ODoiLi8OIESMQERGBESNGIC4uDgBw+vRplJaWIiIiAgsXLkR0dDQAoKamBgcPHsS6deuwbt06HDx4EDU1NQDuzqy9aNEiREREoLS0FFlZWTo5DyIi0o5WwWNra4sPP/wQ27ZtwxdffNHq62HU1tbi3LlzmDRpEgBAJpPB0tIS6enp8PPzA3D3wYb09HQAQEZGBnx9fSGRSODq6opbt26hsrISWVlZcHd3h1wuh1wuh7u7O7KyslBZWYm6ujq4urpCIpHA19dX0xcRERmGVtfOGhoaMGrUKDQ2NqKioqLLDl5WVobevXtjy5Yt+Omnn+Dk5IS5c+eiqqpKM/O1tbU1bt68CQBQqVRQKpWa/RUKBVQqFVQqFRQKhabdxsam3fbm7YmIyHC0Ch5dLfbW1NSEy5cvY968eRgyZAhiYmI0l9Xa0958ph3dr5FIJO1u35H4+HjEx8cDAMLCwrTej4i6n5Z/4HYlmUyms74fJ1o/LVBUVIS0tDRUVVVh/vz5KC4uxp07dzBw4MBOH1yhUEChUGDIkCEAAB8fH8TFxcHKykqzxHZlZSV69+6t2b7lzK4VFRWwtraGjY0N8vLyNO0qlQrDhg3TLOHQcnsbG5t2awkMDERgYGCnz4WIug9dzSDN2anv0uoez8mTJ7FmzRqoVCokJycDAOrq6rBnz57OV4i7C8wpFAoUFxcDAM6ePQtHR0d4eXkhKSkJAJCUlIQxY8YAALy8vJCcnAwhBC5evAgLCwtYW1vDw8MD2dnZqKmpQU1NDbKzs+Hh4QFra2uYm5vj4sWLEEIgOTkZXl5c7oCIyJC0GvEcOHAA7733HgYNGoSTJ08CAAYOHIgrV648dAHz5s1DREQEGhsbYWdnh5CQEAghsHHjRiQkJECpVGLZsmUAAE9PT2RmZuKNN96AiYmJ5hKgXC7HH/7wB7zzzjsAgBdffBFyuRwAsGDBAmzZsgUNDQ3w8PCAp6fnQ9dMRESdp1XwVFVVtbmkJpFIuuTzMIMGDWr3nsrq1avbtEkkEixYsKDdfiZNmqR5Oq4lZ2dnrF+//qHrJCKirqHVpTYnJyfNJbZmJ06cgIuLi06KIiKi7kurEU9QUBA+/PBDJCQk4Pbt21i7di2Ki4uxatUqXddHRETdjFbB4+DggPDwcJw6dQqjR4+GQqHA6NGjYWZmpuv6iIiom9H6cWpTU1OMGzdOl7UQEVEPoNU9HiIioq7C4CEiIr1i8BARkV4xeIiISK+0Xghu9+7duHLlCurr61u9tn//fp0URkRE3ZNWwbNp0yY89dRTCAoK4tLXRET0ULQKnhs3bmDmzJlcMpqIiB6aVvd4/Pz88MMPP+i6FiIi6gG0GvFMmzYNq1atQmxsLKysrFq9tmbNGp0URkRE3ZNWwbNhwwbY2dnB29ub93iIiOihaP1U286dOyGTaT3DDhERUbu0usfj5uaGoqIiXddCREQ9gFZDGFtbW3z44Yfw9vZuc49n5syZOimMiIi6J62Cp6GhAaNGjUJjYyMqKip0XRMREXVjWgVPSEiIrusgIqIeQqvguXbtWoev9e3bt8uKISKi7k+r4HnjjTc6fO2LL77osmKIiKj70yp47g2XGzdu4D//+Q/c3Nx0UhQREXVfnVoWoU+fPpg7dy4+//zzrq6HiIi6uU6vx1NcXIzbt293ZS1ERNQDaHWpbfXq1a1mpr59+zauXr2KF198sUuKUKvVWLFiBWxsbLBixQqUlZUhPDwcNTU1GDx4MF5//XXIZDLcuXMHkZGRKCwsRK9evRAaGgo7OzsAQGxsLBISEmBkZISgoCB4eHgAALKyshATEwO1Wo2AgABMmzatS2omIqLO0Sp4Jk2a1Op7MzMzDBw4EE888USXFHHkyBE4ODigrq4OALBv3z5MnjwZ48ePx7Zt25CQkIDf/va3SEhIgKWlJTZv3owTJ07gs88+w9KlS1FUVITU1FRs2LABlZWV+Pvf/45NmzYBAHbs2IFVq1ZBoVDgnXfegZeXFxwdHbukbiIienBaXWrz9/dv9eXj49NloVNRUYHMzEwEBAQAAIQQyM3NhY+Pj+bY6enpAICMjAz4+/sDAHx8fJCTkwMhBNLT0zFu3DgYGxvDzs4O/fr1Q35+PvLz89GvXz/07dsXMpkM48aN0/RFRESGodWIp7GxEYmJie0ufb1kyZKHKmDXrl14+eWXNaOd6upqWFhYQCqVAgBsbGygUqkAACqVCgqFAgAglUphYWGB6upqqFQqDBkyRNNny32at2/+96VLlx6qXiIiejhaBU9kZCR++uknjB49us1cbQ/j1KlTsLKygpOTE3Jzc391eyFEmzaJRNJu+/22b098fDzi4+MBAGFhYb9aCxF1X0qlUif9ymQynfX9ONEqeLKzsxEZGQlLS8suPfiFCxeQkZGB06dPo6GhAXV1ddi1axdqa2vR1NQEqVQKlUoFGxsbAHdHLBUVFVAoFGhqakJtbS3kcrmmvVnLfVq2V1RUwNraut1aAgMDERgY2KXnR0SPp/Lycp30q1Qqddb3o8De3l6r7bS6x6NUKnHnzp2HKqg9s2fPxtatWxEVFYXQ0FA8+eSTeOONNzB8+HCkpaUBABITE+Hl5QUAGD16NBITEwEAaWlpGD58OCQSCby8vJCamoo7d+6grKwMJSUlcHFxgbOzM0pKSlBWVobGxkakpqZq+iIiIsPQasTj6+uLf/3rX3juuefQp0+fVq89+eSTXV7Un//8Z4SHh+Pf//43Bg8erHmqbtKkSYiMjMTrr78OuVyO0NBQAED//v0xduxYLFu2DEZGRpg/fz6MjO5m6rx587B27Vqo1WpMnDgR/fv37/J6iYhIexLR0Q2SFhYvXtz+zhIJIiMju7yoR8HVyRwZEfVU0u1f66RfXmq7S6sRT1RU1EMVQ0RE1KzTU+YQERF1BoOHiIj0isFDRER6xeAhIiK9YvAQEZFeMXiIiEivGDxERKRXDB4iItIrBg8REekVg4eIiPSKwUNERHrF4CEiIr1i8BARkV4xeIiISK8YPEREpFcMHiIi0isGDxER6RWDh4iI9IrBQ0REesXgISIivWLwEBGRXjF4iIhIrxg8RESkVzJDHry8vBxRUVG4ceMGJBIJAgMD8fzzz6OmpgYbN27E9evXYWtri6VLl0Iul0MIgZiYGJw+fRqmpqYICQmBk5MTACAxMRGHDh0CAMyYMQP+/v4AgMLCQkRFRaGhoQGenp4ICgqCRCIx1CkTEfV4Bh3xSKVSzJkzBxs3bsTatWtx7NgxFBUVIS4uDiNGjEBERARGjBiBuLg4AMDp06dRWlqKiIgILFy4ENHR0QCAmpoaHDx4EOvWrcO6detw8OBB1NTUAAC2b9+ORYsWISIiAqWlpcjKyjLY+RIRkYGDx9raWjNiMTc3h4ODA1QqFdLT0+Hn5wcA8PPzQ3p6OgAgIyMDvr6+kEgkcHV1xa1bt1BZWYmsrCy4u7tDLpdDLpfD3d0dWVlZqKysRF1dHVxdXSGRSODr66vpi4iIDOORucdTVlaGy5cvw8XFBVVVVbC2tgZwN5xu3rwJAFCpVFAqlZp9FAoFVCoVVCoVFAqFpt3Gxqbd9ubtiYjIcAx6j6dZfX091q9fj7lz58LCwqLD7YQQbdo6ul8jkUja3b4j8fHxiI+PBwCEhYVpvR8RdT8t/8DtSjKZTGd9P04MHjyNjY1Yv349JkyYgKeeegoAYGVlhcrKSlhbW6OyshK9e/cGcHfEUl5ertm3oqIC1tbWsLGxQV5enqZdpVJh2LBhUCgUqKioaLW9jY1Nu3UEBgYiMDBQF6dIRI+Zlr9nupJSqdRZ348Ce3t7rbYz6KU2IQS2bt0KBwcHTJkyRdPu5eWFpKQkAEBSUhLGjBmjaU9OToYQAhcvXoSFhQWsra3h4eGB7Oxs1NTUoKamBtnZ2fDw8IC1tTXMzc1x8eJFCCGQnJwMLy8vg5wrERHdJREPcj2qi50/fx6rV6/GgAEDNJfMZs2ahSFDhmDjxo0oLy+HUqnEsmXLNI9T79ixA9nZ2TAxMUFISAicnZ0BAAkJCYiNjQVw93HqiRMnAgAKCgqwZcsWNDQ0wMPDA/PmzdPqceqrkxlQRD2VdPvXOumXI567DBo8jzIGD1HPxeDpnMfiUhsREfU8DB4iItIrBg8REekVg4eIiPSKwUNERHrF4CEiIr1i8BARkV4xeIiISK8YPEREpFcMHiIi0isGDxER6RWDh4iI9IrBQ0REesXgISIivWLwEBGRXjF4iIhIrxg8RESkVwweIiLSKwYPERHpFYOHiIj0isFDRER6xeAhIiK9YvAQEZFeMXiIiEivZIYuQB+ysrIQExMDtVqNgIAATJs2zdAlERH1WN1+xKNWq7Fjxw6sXLkSGzduxIkTJ1BUVGTosoiIeqxuHzz5+fno168f+vbtC5lMhnHjxiE9Pd3QZRER9VjdPnhUKhUUCoXme4VCAZVKZcCKiIh6tm5/j0cI0aZNIpG0aYuPj0d8fDwAICwsDP2/zdB5bUTU89jb2xu6BIPr9iMehUKBiooKzfcVFRWwtrZus11gYCDCwsIQFhaGFStW6LNEveP5Pb6687kBPL/Hnbbn1+2Dx9nZGSUlJSgrK0NjYyNSU1Ph5eVl6LKIiHqsbn+pTSqVYt68eVi7di3UajUmTpyI/v37G7osIqIeq9sHDwCMGjUKo0aN0nr7wMBAHVZjeDy/x1d3PjeA5/e40/b8JKK9u+9EREQ60u3v8RAR0aOlR1xq01Z3n1pny5YtyMzMhJWVFdavX2/ocrpUeXk5oqKicOPGDUgkEgQGBuL55583dFldpqGhAWvWrEFjYyOamprg4+ODl156ydBldSm1Wo0VK1bAxsamWz79tXjxYpiZmcHIyAhSqRRhYWGGLqnL3Lp1C1u3bsXVq1chkUgQHBwMV1fXDrdn8PxP89Q6q1atgkKhwDvvvAMvLy84OjoaurQu4+/vj9/97neIiooydCldTiqVYs6cOXByckJdXR1WrFgBd3f3bvPzMzY2xpo1a2BmZobGxkasXr0aHh4e9/2f+3Fz5MgRODg4oK6uztCl6MyaNWvQu3dvQ5fR5WJiYuDh4YHly5ejsbERt2/fvu/2vNT2Pz1hap1hw4ZBLpcbugydsLa2hpOTEwDA3NwcDg4O3WqGColEAjMzMwBAU1MTmpqa2v0g9OOqoqICmZmZCAgIMHQp9IBqa2tx7tw5TJo0CQAgk8lgaWl533044vmf9qbWuXTpkgEros4qKyvD5cuX4eLiYuhSupRarcbbb7+N0tJSPPvssxgyZIihS+oyu3btwssvv9ytRzsAsHbtWgDAM888022ecCsrK0Pv3r2xZcsW/PTTT3BycsLcuXM1fyi1hyOe/9F2ah16tNXX12P9+vWYO3cuLCwsDF1OlzIyMsK//vUvbN26FQUFBfj5558NXVKXOHXqFKysrDQj1u7q73//Oz766COsXLkSx44dQ15enqFL6hJNTU24fPkyfvvb3+Kf//wnTE1NERcXd999GDz/o+3UOvToamxsxPr16zFhwgQ89dRThi5HZywtLTFs2DBkZWUZupQuceHCBWRkZGDx4sUIDw9HTk4OIiIiDF1Wl7OxsQEAWFlZYcyYMcjPzzdwRV1DoVBAoVBoRuA+Pj64fPnyffdh8PwPp9Z5vAkhsHXrVjg4OGDKlCmGLqfL3bx5E7du3QJw9wm3s2fPwsHBwcBVdY3Zs2dj69atiIqKQmhoKJ588km88cYbhi6rS9XX12suI9bX1+PMmTMYMGCAgavqGn369IFCoUBxcTEA4OzZs7/6UA/v8fxPT5haJzw8HHl5eaiursZrr72Gl156SXND8HF34cIFJCcnY8CAAXjrrbcAALNmzXqgGSseZZWVlYiKioJarYYQAmPHjsXo0aMNXRZpqaqqCh9//DGAu5emnn76aXh4eBi4qq4zb948REREoLGxEXZ2dggJCbnv9py5gIiI9IqX2oiISK8YPEREpFcMHiIi0isGDxER6RWDh4iI9IrBQ6RH//73vzF//ny8+uqr7b7+/fff49VXX8WcOXNQXV19374SExPx3nvvab5/6aWXUFpaqnUthw4dwtatW7Xenqir8HM8RPfYtWsXkpKSYG9vj+XLl2s+cZ6SkoL8/HwEBQV1qt/y8nJ888032LJlC6ysrNq83tjYiN27d2Pt2rUYNGjQw5xCG7m5udi8eXOroJkxY0aXHoNIWxzxELWQn5+PwsJCbNu2DUOHDkVsbCyAuzPwfvPNN5g5c2an+y4vL0evXr3aDR3g7ocM79y50+0+uEx0L454iFooKyvD0KFDYWxsjBEjRuDo0aMAgP3792Pq1Km/OvFobW0tdu7cidOnT8PU1BQBAQGYPn06cnJy8NFHH6GxsRFz5syBj48PFi9erNmvuLgYb7/9NgBg7ty5cHFxQXBwMJYsWYL9+/dDKpUCAN5//31MmDDhgZYPqK+vx7p16zTHBoBNmzYhPj4epaWleOONN1BWVoYlS5YgODgYBw4cQH19PWbNmgUnJyds3boV5eXlmDBhAubPn6/pNyEhAd988w1u3LgBFxcXLFy4ELa2tlrXRT0XRzxELTg6OuLcuXOa+dAcHR1RUFCA4uJiPP3007+6/86dO1FbW4vIyEi8//77SE5ORmJiItzd3bFy5UpYW1tj7969rUIHAOzt7TWrwu7atQtr1qzpsnMyMzNrdey9e/dqLh/e69KlS9i0aRNCQ0Oxe/duHDp0CO+99x42bNiAkydPamZU/r//+z/ExsZi+fLliI6OxtChQ7Fp06Yuq5m6NwYPUQsDBgzAU089hXfffRfl5eV44YUXsGvXLgQFBeHIkSNYs2YNIiIiNBN2tqRWq5GamorZs2fD3NwcdnZ2mDJlCpKTkw1wJp3z4osvwsTEBCNHjoSpqSmefvppWFlZwcbGBkOHDtXMOhwfH4/p06fD0dERUqkU06dPx5UrV3D9+nUDnwE9DnipjegeU6ZM0cxw/d1332Ho0KEQQuC///0vPvroI3z11VeIi4vDn//851b73bx5E42NjVAqlZo2W1tbva+EWl5ejqVLl2q+37t3r9b7trz/ZGJi0ub7+vp6AMD169cRExODPXv2aF4XQkClUvFyG/0qBg9RB27cuIH4+HisXbsWp06dwoABAyCTyeDs7Ky599NS7969IZVKUV5erpkWvry8vMPLWr+meQXH27dva+4t3bhx41f3UyqVbcKmqxc1VCqVmDFjBiZMmNCl/VLPwEttRB3Ys2cPXnrpJZiamsLOzg4FBQWor69HXl4e7Ozs2mxvZGSEsWPHYv/+/airq8P169dx+PDhTv9y7t27N2xsbJCSkgK1Wo2EhARcu3atU31ZWVmhuroatbW1ndr/Xs888wzi4uJw9epVAHcfqjh58mSX9E3dH0c8RO3IycnBrVu34O3tDQBwcXHBqFGjEBwcDHt7eyxbtqzd/ebNm4edO3diyZIlMDExQUBAACZOnNjpOhYtWoTo6Gjs378fkyZNgqura6f6cXBwwPjx47FkyRKo1Wps2LCh0zUBgLe3N+rr6xEeHo7y8nJYWFhgxIgRGDt27EP1Sz0D1+MhIiK94qU2IiLSKwYPERHpFYOHiIj0isFDRER6xeAhIiK9YvAQEZFeMXiIiEivGDxERKRXDB4iItKr/wdSxRwyg5+U3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarizing the data\n",
    "\n",
    "# Print the summary statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(df['FTE'].dropna())\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of %full-time \\n employee works')\n",
    "plt.xlabel('% of full-time')\n",
    "plt.ylabel('num employees')\n",
    "plt.xlim([0,6])\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why encode labels as categories?\n",
    "    - ML algorithms work on numbers, not strings\n",
    "    - Strings can be slow compared to numbers (take more space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b\n",
       "1    b\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode labels as categories (sample data)\n",
    "\n",
    "sample_df.label.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b\n",
       "1    b\n",
       "Name: label, dtype: category\n",
       "Categories (2, object): [a, b]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.label = sample_df.label.astype('category')\n",
    "sample_df.label.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_a</th>\n",
       "      <th>label_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_a  label_b\n",
       "0        0        1\n",
       "1        0        1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy variable encoding\n",
    "\n",
    "dummies = pd.get_dummies(sample_df[['label']], prefix_sep='_')\n",
    "dummies.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lambda functions\n",
    "\n",
    "square = lambda x: x*x\n",
    "square(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      "numeric         1000 non-null float64\n",
      "text            1000 non-null object\n",
      "with_missing    822 non-null float64\n",
      "label           1000 non-null category\n",
      "dtypes: category(1), float64(2), object(1)\n",
      "memory usage: 32.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Encode labels as categories\n",
    "\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "sample_df[['label']] = sample_df[['label']].apply(categorize_label,axis=0)\n",
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     23\n",
       "float64     2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring datatypes in pandas\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels as categorical variables\n",
    "LABELS = ['Function','Use','Sharing','Reporting','Student_Type',\n",
    "          'Position_Type','Object_Type','Pre_K','Operating_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function            object\n",
       "Use                 object\n",
       "Sharing             object\n",
       "Reporting           object\n",
       "Student_Type        object\n",
       "Position_Type       object\n",
       "Object_Type         object\n",
       "Pre_K               object\n",
       "Operating_Status    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[LABELS].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function            category\n",
      "Use                 category\n",
      "Sharing             category\n",
      "Reporting           category\n",
      "Student_Type        category\n",
      "Position_Type       category\n",
      "Object_Type         category\n",
      "Pre_K               category\n",
      "Operating_Status    category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Define the lambda function: categorize_label\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "\n",
    "# Convert df[LABELS] to a categorical type\n",
    "df[LABELS] = df[LABELS].apply(categorize_label,axis=0)\n",
    "\n",
    "# Print the converted dtypes\n",
    "print(df[LABELS].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFZCAYAAAB33zMcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b9fc718f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Counting unique labels\n",
    "\n",
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate number of unique values for each label: num_unique_labels\n",
    "num_unique_labels = df[LABELS].apply(lambda x: pd.Series.nunique(x))\n",
    "\n",
    "# Plot number of unique values for each label\n",
    "num_unique_labels.plot(kind='bar')\n",
    "\n",
    "# Label the axes\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Number of unique values')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we measure success?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log loss binary classification  \n",
    "logloss(N=1) = y log(p) + (1-y) log(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing log loss with NumPy\n",
    "\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    \"\"\" Computes the logarithmic loss between predicted and\n",
    "        actual when these are 1D arrays.\n",
    "    \n",
    "    :param predicted: The predicted probabilities as floats between 0-1\n",
    "    :param actual: The actual binary labels. Either 0 or 1.\n",
    "    :param eps (optional): log(0) is inf, so we need to offset our\n",
    "                           predicted values slightly by eps from 0 or 1.\n",
    "    \"\"\"\n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted)\n",
    "                        + (1 - actual) * np.log(1 - predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_log_loss(predicted=0.9, actual=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69314718055994529"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_log_loss(predicted=0.5, actual=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0.16251892949777494\n",
      "B: 4.605170185988091\n",
      "C: 0.7133498878774648\n"
     ]
    }
   ],
   "source": [
    "# Penalizing highly confident wrong answers\n",
    "print('A: {}'.format(compute_log_loss(predicted=0.85, actual=1)))\n",
    "print('B: {}'.format(compute_log_loss(predicted=0.99, actual=0)))\n",
    "print('C: {}'.format(compute_log_loss(predicted=0.51, actual=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing log loss with NumPy\n",
    "# 5 one-dimensional numeric arrays simulating different types of predictions\n",
    "\n",
    "actual_labels = np.array([ 1.,  1.,  1.,  1.,  1.,  \n",
    "                          0.,  0.,  0.,  0.,  0.])\n",
    "correct_confident = np.array([ 0.95,  0.95,  0.95,  0.95,  0.95,  \n",
    "                              0.05,  0.05,  0.05,  0.05,  0.05])\n",
    "correct_not_confident = np.array([ 0.65,  0.65,  0.65,  0.65,  0.65,  \n",
    "                                  0.35,  0.35,  0.35,  0.35,  0.35])\n",
    "wrong_not_confident = np.array([ 0.35,  0.35,  0.35,  0.35,  0.35,  \n",
    "                                0.65,  0.65,  0.65,  0.65,  0.65])\n",
    "wrong_confident = np.array([ 0.05,  0.05,  0.05,  0.05,  0.05,  \n",
    "                            0.95,  0.95,  0.95,  0.95,  0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss, correct and confident: 0.05129329438755058\n",
      "Log loss, correct and not confident: 0.4307829160924542\n",
      "Log loss, wrong and not confident: 1.049822124498678\n",
      "Log loss, wrong and confident: 2.9957322735539904\n",
      "Log loss, actual labels: 9.99200722162646e-15\n"
     ]
    }
   ],
   "source": [
    "# Compute and print log loss for 1st case\n",
    "correct_confident = compute_log_loss(correct_confident, actual_labels)\n",
    "print(\"Log loss, correct and confident: {}\".format(correct_confident)) \n",
    "\n",
    "# Compute log loss for 2nd case\n",
    "correct_not_confident = compute_log_loss(correct_not_confident, actual_labels)\n",
    "print(\"Log loss, correct and not confident: {}\".format(correct_not_confident)) \n",
    "\n",
    "# Compute and print log loss for 3rd case\n",
    "wrong_not_confident = compute_log_loss(wrong_not_confident, actual_labels)\n",
    "print(\"Log loss, wrong and not confident: {}\".format(wrong_not_confident)) \n",
    "\n",
    "# Compute and print log loss for 4th case\n",
    "wrong_confident = compute_log_loss(wrong_confident, actual_labels)\n",
    "print(\"Log loss, wrong and confident: {}\".format(wrong_confident)) \n",
    "\n",
    "# Compute and print log loss for actual labels\n",
    "actual_labels = compute_log_loss(actual_labels, actual_labels)\n",
    "print(\"Log loss, actual labels: {}\".format(actual_labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 2: Creating a simple first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from warnings import warn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter:  \n",
    "- Build first-pass model based only on numeric data\n",
    "- Multi-class logistic regression\n",
    "- Format predictions and save to csv\n",
    "- Compute log-loss score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time to build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**multilabel_train_test_split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# DEFINITION OF multilabel_train_test_split\n",
    "###########################################\n",
    "\n",
    "def multilabel_sample(y, size=1000, min_count=5, seed=None):\n",
    "    \"\"\" Takes a matrix of binary labels `y` and returns\n",
    "        the indices for a sample of size `size` if\n",
    "        `size` > 1 or `size` * len(y) if size =< 1.\n",
    "        The sample is guaranteed to have > `min_count` of\n",
    "        each label.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (np.unique(y).astype(int) != np.array([0, 1])).all():\n",
    "            raise ValueError()\n",
    "    except (TypeError, ValueError):\n",
    "        raise ValueError('multilabel_sample only works with binary indicator matrices')\n",
    "\n",
    "    if (y.sum(axis=0) < min_count).any():\n",
    "        raise ValueError('Some classes do not have enough examples. Change min_count if necessary.')\n",
    "\n",
    "    if size <= 1:\n",
    "        size = np.floor(y.shape[0] * size)\n",
    "\n",
    "    if y.shape[1] * min_count > size:\n",
    "        msg = \"Size less than number of columns * min_count, returning {} items instead of {}.\"\n",
    "        warn(msg.format(y.shape[1] * min_count, size))\n",
    "        size = y.shape[1] * min_count\n",
    "\n",
    "    rng = np.random.RandomState(seed if seed is not None else np.random.randint(1))\n",
    "\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        choices = y.index\n",
    "        y = y.values\n",
    "    else:\n",
    "        choices = np.arange(y.shape[0])\n",
    "\n",
    "    sample_idxs = np.array([], dtype=choices.dtype)\n",
    "\n",
    "    # first, guarantee > min_count of each label\n",
    "    for j in range(y.shape[1]):\n",
    "        label_choices = choices[y[:, j] == 1]\n",
    "        label_idxs_sampled = rng.choice(label_choices, size=min_count, replace=False)\n",
    "        sample_idxs = np.concatenate([label_idxs_sampled, sample_idxs])\n",
    "\n",
    "    sample_idxs = np.unique(sample_idxs)\n",
    "\n",
    "    # now that we have at least min_count of each, we can just random sample\n",
    "    sample_count = int(size - sample_idxs.shape[0])\n",
    "\n",
    "    # get sample_count indices from remaining choices\n",
    "    remaining_choices = np.setdiff1d(choices, sample_idxs)\n",
    "    remaining_sampled = rng.choice(remaining_choices,\n",
    "                                   size=sample_count,\n",
    "                                   replace=False)\n",
    "    return np.concatenate([sample_idxs, remaining_sampled])\n",
    "\n",
    "def multilabel_sample_dataframe(df, labels, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a dataframe `df` and returns a sample of size `size` where all\n",
    "        classes in the binary matrix `labels` are represented at\n",
    "        least `min_count` times.\n",
    "    \"\"\"\n",
    "    idxs = multilabel_sample(labels, size=size, min_count=min_count, seed=seed)\n",
    "    return df.loc[idxs]\n",
    "\n",
    "\n",
    "def multilabel_train_test_split(X, Y, size, min_count=5, seed=None):\n",
    "    \"\"\" Takes a features matrix `X` and a label matrix `Y` and\n",
    "        returns (X_train, X_test, Y_train, Y_test) where all\n",
    "        classes in Y are represented at least `min_count` times.\n",
    "    \"\"\"\n",
    "    index = Y.index if isinstance(Y, pd.DataFrame) else np.arange(Y.shape[0])\n",
    "\n",
    "    test_set_idxs = multilabel_sample(Y, size=size, min_count=min_count, seed=seed)\n",
    "    train_set_idxs = np.setdiff1d(index, test_set_idxs)\n",
    "\n",
    "    test_set_mask = index.isin(test_set_idxs)\n",
    "    train_set_mask = ~test_set_mask\n",
    "\n",
    "    return (X[train_set_mask], X[test_set_mask], Y[train_set_mask], Y[test_set_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the multi-class dataset\n",
    "\n",
    "NUMERIC_COLUMNS = ['FTE','Total']\n",
    "LABELS = ['Function','Use','Sharing','Reporting','Student_Type',\n",
    "          'Position_Type','Object_Type','Pre_K','Operating_Status']\n",
    "data_to_train = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "labels_to_use = pd.get_dummies(df[LABELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    data_to_train, labels_to_use,size=0.2, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Data columns (total 2 columns):\n",
      "FTE      320222 non-null float64\n",
      "Total    320222 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 7.3 MB\n",
      "None\n",
      "\n",
      "X_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 72072\n",
      "Data columns (total 2 columns):\n",
      "FTE      80055 non-null float64\n",
      "Total    80055 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 1.8 MB\n",
      "None\n",
      "\n",
      "y_train info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 320222 entries, 134338 to 415831\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 34.2 MB\n",
      "None\n",
      "\n",
      "y_test info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 80055 entries, 206341 to 72072\n",
      "Columns: 104 entries, Function_Aides Compensation to Operating_Status_PreK-12 Operating\n",
      "dtypes: uint8(104)\n",
      "memory usage: 8.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Setting up a train-test split in scikit-learn\n",
    "\n",
    "# Create the new DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    numeric_data_only,label_dummies,size=0.2,seed=123)\n",
    "\n",
    "# Print the info\n",
    "print(\"X_train info:\")\n",
    "print(X_train.info())\n",
    "print(\"\\nX_test info:\")  \n",
    "print(X_test.info())\n",
    "print(\"\\ny_train info:\")  \n",
    "print(y_train.info())\n",
    "print(\"\\ny_test info:\")  \n",
    "print(y_test.info()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Training a model\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Create the DataFrame: numeric_data_only\n",
    "numeric_data_only = df[NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "# Get labels and convert to dummy variables: label_dummies\n",
    "label_dummies = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    numeric_data_only,label_dummies,size=0.2,seed=123)\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50064, 104)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout = pd.read_csv('datasets/drivendata/TestData.csv',index_col=0,dtype='str')\n",
    "holdout = holdout[NUMERIC_COLUMNS].fillna(-1000)\n",
    "predictions = clf.predict_proba(holdout)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame(\n",
    "    columns=pd.get_dummies(df[LABELS],prefix_sep='__').columns,\n",
    "    index=holdout.index,\n",
    "    data=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv('datasets/drivendata/predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT AVAILABLE: Method score_submission internal to DataCamp\n",
    "#score = score_submission(pred_path='predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your model to predict values on holdout data\n",
    "\n",
    "# Instantiate the classifier: clf\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "# Fit it to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Load the holdout data: holdout\n",
    "holdout = pd.read_csv('datasets/drivendata/TestData.csv',index_col=0)\n",
    "\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing out your results to a csv for submission\n",
    "# NOT AVAILABLE: Method score_submission internal to DataCamp.\n",
    "\n",
    "# Generate predictions: predictions\n",
    "predictions = clf.predict_proba(holdout[NUMERIC_COLUMNS].fillna(-1000))\n",
    "\n",
    "# Format predictions in DataFrame: prediction_df\n",
    "prediction_df = pd.DataFrame(\n",
    "    columns=pd.get_dummies(df[LABELS]).columns,\n",
    "    index=holdout.index,data=predictions)\n",
    "\n",
    "# Save prediction_df to csv\n",
    "prediction_df.to_csv('predictions.csv')\n",
    "\n",
    "# Submit the predictions for scoring: score\n",
    "#score = score_submission(pred_path='predictions.csv')\n",
    "\n",
    "# Print score\n",
    "#print('Your model, trained with numeric data only, yields logloss score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very brief introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: Splitting a string into segments  \n",
    "Bag of words representation: Count the number of times a particular token appears  \n",
    "n-gram representation: creates ordered groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing text numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words\n",
    "- Simple way to represent text in machine learning\n",
    "- Discards information about grammar and word order\n",
    "- Computes frequency of occurrence  \n",
    "\n",
    "CountVectorizer()\n",
    "- Tokenizes all the strings\n",
    "- Builds a vocabulary\n",
    "- Counts the occurrences of each token in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CountVectorizer() on column of main dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "df.Program_Description.fillna('', inplace=True)\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='\\\\S+(?=\\\\s+)', tokenizer=None,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_basic.fit(df.Program_Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 434 tokens in Program_Description if tokens are any non-whitespace\n"
     ]
    }
   ],
   "source": [
    "msg = 'There are {} tokens in Program_Description if tokens are any non-whitespace'\n",
    "print(msg.format(len(vec_basic.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 385 tokens in Position_Extra if we split on non-alpha numeric\n",
      "['1st', '2nd', '3rd', '4th', '56', '5th', '9th', 'a', 'ab', 'accountability', 'adaptive', 'addit', 'additional', 'adm', 'admin']\n"
     ]
    }
   ],
   "source": [
    "# Creating a bag-of-words in scikit-learn\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Fill missing values in df.Position_Extra\n",
    "df.Position_Extra.fillna('',inplace=True)\n",
    "\n",
    "# Instantiate the CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit to the data\n",
    "vec_alphanumeric.fit(df.Position_Extra)\n",
    "\n",
    "# Print the number of tokens and first 15 tokens\n",
    "msg = \"There are {} tokens in Position_Extra if we split on non-alpha numeric\"\n",
    "print(msg.format(len(vec_alphanumeric.get_feature_names())))\n",
    "print(vec_alphanumeric.get_feature_names()[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining text columns for tokenization\n",
    "\n",
    "# Define combine_text_columns()\n",
    "def combine_text_columns(data_frame, to_drop=NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n",
    "    text_data = data_frame.drop(to_drop,axis=1)\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    text_data.fillna('',inplace=True)\n",
    "    \n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4758 tokens in the dataset\n",
      "There are 3284 alpha-numeric tokens in the dataset\n"
     ]
    }
   ],
   "source": [
    "# What's in a token?\n",
    "# Apply the above created function 'combine_text_columns'\n",
    "# on the dataframe\n",
    "\n",
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the basic token pattern\n",
    "TOKENS_BASIC = '\\\\S+(?=\\\\s+)'\n",
    "\n",
    "# Create the alphanumeric token pattern\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate basic CountVectorizer: vec_basic\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC)\n",
    "\n",
    "# Instantiate alphanumeric CountVectorizer: vec_alphanumeric\n",
    "vec_alphanumeric = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(df)\n",
    "\n",
    "# Fit and transform vec_basic\n",
    "vec_basic.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_basic\n",
    "print(\"There are {} tokens in the dataset\".format(len(vec_basic.get_feature_names())))\n",
    "\n",
    "# Fit and transform vec_alphanumeric\n",
    "vec_alphanumeric.fit_transform(text_vector)\n",
    "\n",
    "# Print number of tokens of vec_alphanumeric\n",
    "print(\"There are {} alpha-numeric tokens in the dataset\".format(len(vec_alphanumeric.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 3: Improving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines, feature & text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate simple pipeline with one step\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "pl = Pipeline([('clf', OneVsRestClassifier(LogisticRegression()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric</th>\n",
       "      <th>text</th>\n",
       "      <th>with_missing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.856306</td>\n",
       "      <td></td>\n",
       "      <td>4.433240</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.973454</td>\n",
       "      <td>foo</td>\n",
       "      <td>4.310229</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.829785</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>2.469828</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-15.062947</td>\n",
       "      <td></td>\n",
       "      <td>2.852981</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.786003</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>1.826475</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     numeric     text  with_missing label\n",
       "0 -10.856306               4.433240     b\n",
       "1   9.973454      foo      4.310229     b\n",
       "2   2.829785  foo bar      2.469828     a\n",
       "3 -15.062947               2.852981     b\n",
       "4  -5.786003  foo bar      1.826475     a"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric']],pd.get_dummies(sample_df['label']),\n",
    "    random_state=2)\n",
    "\n",
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on numeric data, no nans:  0.652\n"
     ]
    }
   ],
   "source": [
    "accuracy = pl.score(X_test, y_test)\n",
    "print('accuracy on numeric data, no nans: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAD STEP: Will not run, need imputer\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric','with_missing']], \n",
    "    pd.get_dummies(sample_df['label']), random_state=2)\n",
    "\n",
    "#pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric','with_missing']], \n",
    "    pd.get_dummies(sample_df['label']), random_state=2)\n",
    "\n",
    "pl = Pipeline([('imp', Imputer()),\n",
    "               ('clf', OneVsRestClassifier(LogisticRegression()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on all numeric, incl nans:  0.648\n"
     ]
    }
   ],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print('accuracy on all numeric, incl nans: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - numeric, no nans:  0.62\n"
     ]
    }
   ],
   "source": [
    "# Instantiate pipeline\n",
    "# trains using the numeric column of the sample data\n",
    "\n",
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import other necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Split and select numeric data only, no nans \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric']],pd.get_dummies(sample_df['label']), \n",
    "    random_state=22)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - numeric, no nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all numeric, incl nans:  0.636\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing numeric features\n",
    "# Include all numeric features, add imputer for missing values\n",
    "\n",
    "# Import the Imputer object\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Create training and test sets using only numeric data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric', 'with_missing']],\n",
    "    pd.get_dummies(sample_df['label']),random_state=456)\n",
    "\n",
    "# Insantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "        ('imp', Imputer()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pl.fit(X_train,y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test,y_test)\n",
    "print(\"\\nAccuracy on sample data - all numeric, incl nans: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features and feature unions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numeric</th>\n",
       "      <th>text</th>\n",
       "      <th>with_missing</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.856306</td>\n",
       "      <td></td>\n",
       "      <td>4.433240</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.973454</td>\n",
       "      <td>foo</td>\n",
       "      <td>4.310229</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.829785</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>2.469828</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-15.062947</td>\n",
       "      <td></td>\n",
       "      <td>2.852981</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.786003</td>\n",
       "      <td>foo bar</td>\n",
       "      <td>1.826475</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     numeric     text  with_missing label\n",
       "0 -10.856306               4.433240     b\n",
       "1   9.973454      foo      4.310229     b\n",
       "2   2.829785  foo bar      2.469828     a\n",
       "3 -15.062947               2.852981     b\n",
       "4  -5.786003  foo bar      1.826475     a"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing text features\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df['text'],pd.get_dummies(sample_df['label']),random_state=2)\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on sample data:  0.848\n"
     ]
    }
   ],
   "source": [
    "pl.fit(X_train, y_train)\n",
    "\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print('accuracy on sample data: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "# Processing numeric and text data together\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric','with_missing', 'text']], \n",
    "    pd.get_dummies(sample_df['label']), random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation step\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "get_text_data = FunctionTransformer(\n",
    "    lambda x: x['text'],validate=False)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(\n",
    "    lambda x: x[['numeric','with_missing']], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union step (Only to show here. Used again next)\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "#union = FeatureUnion([\n",
    "#    ('numeric', numeric_pipeline),\n",
    "#    ('text', text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUTTING IT ALL TOGETHER\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('selector', get_numeric_data),\n",
    "    ('imputer', Imputer())])\n",
    "\n",
    "text_pipeline = Pipeline([\n",
    "    ('selector', get_text_data),\n",
    "    ('vectorizer', CountVectorizer())])\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('numeric', numeric_pipeline),\n",
    "        ('text', text_pipeline)])),\n",
    "    \n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on sample data:  0.936\n"
     ]
    }
   ],
   "source": [
    "# Calling the pipeline\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print('accuracy on sample data: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - just text data:  0.808\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing text features\n",
    "\n",
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Split out only the text data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df['text'],pd.get_dummies(sample_df['label']),random_state=456)\n",
    "\n",
    "# Instantiate Pipeline object: pl\n",
    "pl = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train,y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test,y_test)\n",
    "print(\"\\nAccuracy on sample data - just text data: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data\n",
      "0           \n",
      "1        foo\n",
      "2    foo bar\n",
      "3           \n",
      "4    foo bar\n",
      "Name: text, dtype: object\n",
      "\n",
      "Numeric Data\n",
      "     numeric  with_missing\n",
      "0 -10.856306      4.433240\n",
      "1   9.973454      4.310229\n",
      "2   2.829785      2.469828\n",
      "3 -15.062947      2.852981\n",
      "4  -5.786003      1.826475\n"
     ]
    }
   ],
   "source": [
    "# Multiple types of processing: FunctionTransformer\n",
    "\n",
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Obtain the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "\n",
    "# Obtain the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)\n",
    "\n",
    "# Fit and transform the text data: just_text_data\n",
    "just_text_data = get_text_data.fit_transform(sample_df)\n",
    "\n",
    "# Fit and transform the numeric data: just_numeric_data\n",
    "just_numeric_data = get_numeric_data.fit_transform(sample_df)\n",
    "\n",
    "# Print head to check results\n",
    "print('Text Data')\n",
    "print(just_text_data.head())\n",
    "print('\\nNumeric Data')\n",
    "print(just_numeric_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on sample data - all data:  0.928\n"
     ]
    }
   ],
   "source": [
    "# Multiple types of processing: FeatureUnion\n",
    "\n",
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Split using ALL data in sample_df\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sample_df[['numeric', 'with_missing', 'text']],\n",
    "    pd.get_dummies(sample_df['label']),random_state=22)\n",
    "\n",
    "# Create a FeatureUnion with nested pipeline: process_and_join_features\n",
    "process_and_join_features = FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )\n",
    "\n",
    "# Instantiate nested pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', process_and_join_features),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "\n",
    "# Fit pl to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on sample data - all data: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main dataset: lots of text\n",
    "\n",
    "LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type',\n",
    "          'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']\n",
    "\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "len(NON_LABELS) - len(NUMERIC_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline with the main dataset\n",
    "\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    df[NON_LABELS], dummy_labels,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(\n",
    "    combine_text_columns,validate=False)\n",
    "\n",
    "get_numeric_data = FunctionTransformer(\n",
    "    lambda x:x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion([\n",
    "        ('numeric_features', Pipeline([\n",
    "            ('selector', get_numeric_data),\n",
    "            ('imputer', Imputer())\n",
    "        ])),\n",
    "        ('text_features', Pipeline([\n",
    "            ('selector', get_text_data),\n",
    "            ('vectorizer', CountVectorizer())\n",
    "        ]))\n",
    "    ])\n",
    "    ),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on real data, using Logistic Regression:  0.379863843608\n"
     ]
    }
   ],
   "source": [
    "# Performance using Logistic Regression\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on real data, using Logistic Regression: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easily try new models using pipeline\n",
    "# Use Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pl = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list = [\n",
    "            ('numeric_features', Pipeline([\n",
    "                ('selector', get_numeric_data),\n",
    "                ('imputer', Imputer())\n",
    "            ])),\n",
    "            ('text_features', Pipeline([\n",
    "                ('selector', get_text_data),\n",
    "                ('vectorizer', CountVectorizer())\n",
    "            ]))\n",
    "        ]\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on real data, using Random Forest:  0.900181125476\n"
     ]
    }
   ],
   "source": [
    "# Performance using Random Forest\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on real data, using Random Forest: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using FunctionTransformer on the main dataset\n",
    "\n",
    "# Import FunctionTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Get the dummy encoding of the labels\n",
    "dummy_labels = pd.get_dummies(df[LABELS])\n",
    "\n",
    "# Get the columns that are features in the original df\n",
    "NON_LABELS = [c for c in df.columns if c not in LABELS]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = multilabel_train_test_split(\n",
    "    df[NON_LABELS], dummy_labels, 0.2, seed=123)\n",
    "\n",
    "# Preprocess the text data: get_text_data\n",
    "get_text_data = FunctionTransformer(\n",
    "    combine_text_columns,validate=False)\n",
    "\n",
    "# Preprocess the numeric data: get_numeric_data\n",
    "get_numeric_data = FunctionTransformer(\n",
    "    lambda x: x[NUMERIC_COLUMNS], validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.370895009681\n"
     ]
    }
   ],
   "source": [
    "# Add a model to the pipeline\n",
    "\n",
    "# Complete the pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train,y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.904615576791\n"
     ]
    }
   ],
   "source": [
    "# Try a different class of model\n",
    "\n",
    "# Import random forest classifer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Edit model step in pipeline\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on budget dataset:  0.913359565299\n"
     ]
    }
   ],
   "source": [
    "# Adjust model/parameters to Improve accuracy\n",
    "\n",
    "# Import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Add model step to pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer())\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('clf', RandomForestClassifier(n_estimators=15))\n",
    "    ])\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print accuracy\n",
    "accuracy = pl.score(X_test, y_test)\n",
    "print(\"\\nAccuracy on budget dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chap 4: Learning from the experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Chapter introduces the Log-Loss evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# DEFINITION OF compute_log_loss \n",
    "# STILL HAVE TO OBSERVE HOW TO USE IT FOR \n",
    "# MULTICLASS CLASSIFICATION\n",
    "###########################################\n",
    "def compute_log_loss(predicted, actual, eps=1e-14):\n",
    "    \"\"\" Computes the logarithmic loss between predicted and\n",
    "    actual when these are 1D arrays.\n",
    "    :param predicted: The predicted probabilities as floats between 0-1\n",
    "    :param actual: The actual binary labels. Either 0 or 1.\n",
    "    :param eps (optional): log(0) is inf, so we need to offset our\n",
    "    predicted values slightly by eps from 0 or 1.\n",
    "    \"\"\"\n",
    "    predicted = np.clip(predicted, eps, 1 - eps)\n",
    "    loss = -1 * np.mean(actual * np.log(predicted) + \n",
    "                        (1 - actual) * np.log(1 - predicted))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from the expert: processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams and tokenization\n",
    "txt = 'PETRO-VEND FUEL AND FLUIDS'\n",
    "vec = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many tokens?\n",
    "SAMPLE_STRING = \"'PLANNING,RES,DEV,& EVAL\"\n",
    "# Total tokens = 4, because , and & are not tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00a', '12', '1st', '2nd', '3rd', '4th', '5', '56', '5th', '6']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3171"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deciding what's a word\n",
    "\n",
    "# Import the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create the text vector\n",
    "text_vector = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate the CountVectorizer: text_features\n",
    "text_features = CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC)\n",
    "\n",
    "# Fit text_features to the text vector\n",
    "text_features.fit(text_vector)\n",
    "\n",
    "# Print the first 10 tokens\n",
    "print(text_features.get_feature_names()[:10])\n",
    "len(text_features.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-gram range in scikit-learn\n",
    "\n",
    "# Import pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import other preprocessing modules\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "\n",
    "# Select 300 best features\n",
    "chi_k = 300\n",
    "\n",
    "# Import functional utilities\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Perform preprocessing\n",
    "get_text_data = FunctionTransformer(\n",
    "    combine_text_columns, validate=False)\n",
    "get_numeric_data = FunctionTransformer(\n",
    "    lambda x: x[NUMERIC_COLUMNS], validate=False)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)'\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(\n",
    "                        token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                        ngram_range=(1,2)\n",
    "                    )),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log loss score:  22.963139039\n"
     ]
    }
   ],
   "source": [
    "# ADD LOG-LOSS EVALUATION\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print log-loss score\n",
    "y_pred = pl.predict_proba(X_test)\n",
    "score = log_loss(y_test, y_pred, eps=1e-14)\n",
    "print(\"\\nLog loss score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course exercise shows the log-loss score of 1.2681. The full dataset is giving different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from the expert: a stats trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2\n",
       "a   0   1\n",
       "b   1   1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding interaction features with scikit-learn\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = pd.DataFrame({'x1':[0,1],'x2':[1,1]},index=['a','b'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction = PolynomialFeatures(\n",
    "    degree=2,interaction_only=True,include_bias=False)\n",
    "interaction.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# DEFINITION OF SparseInteractions\n",
    "###########################################\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import sparse\n",
    "from itertools import combinations\n",
    "\n",
    "class SparseInteractions(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, degree=2, feature_name_separator=\"_\"):\n",
    "        self.degree = degree\n",
    "        self.feature_name_separator = feature_name_separator\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        if not sparse.isspmatrix_csc(X):\n",
    "            X = sparse.csc_matrix(X)\n",
    "            \n",
    "        if hasattr(X, \"columns\"):\n",
    "            self.orig_col_names = X.columns\n",
    "        else:\n",
    "            self.orig_col_names = np.array([str(i) for i in range(X.shape[1])])\n",
    "            \n",
    "        spi = self._create_sparse_interactions(X)\n",
    "        return spi\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def _create_sparse_interactions(self, X):\n",
    "        out_mat = []\n",
    "        self.feature_names = self.orig_col_names.tolist()\n",
    "        \n",
    "        for sub_degree in range(2, self.degree + 1):\n",
    "            for col_ixs in combinations(range(X.shape[1]), sub_degree):\n",
    "                # add name for new column\n",
    "                name = self.feature_name_separator.join(self.orig_col_names[list(col_ixs)])\n",
    "                self.feature_names.append(name)\n",
    "                \n",
    "                # get column multiplications value\n",
    "                out = X[:, col_ixs[0]]    \n",
    "                for j in col_ixs[1:]:\n",
    "                    out = out.multiply(X[:, j])\n",
    "\n",
    "                out_mat.append(out)\n",
    "\n",
    "        return sparse.hstack([X] + out_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparseInteractions(degree=2).fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement interaction modeling in scikit-learn\n",
    "\n",
    "# Instantiate pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                                                   ngram_range=(1, 2))),  \n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log loss score:  21.3401234138\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION: COMPUTATIONALLY EXPENSIVE\n",
    "# ADD LOG-LOSS EVALUATION\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print log-loss score\n",
    "y_pred = pl.predict_proba(X_test)\n",
    "score = log_loss(y_test, y_pred, eps=1e-14)\n",
    "print(\"\\nLog loss score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course exercise shows a log-loss score of 1.2256 which is an improvement from the previous value of 1.2681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from the expert: a computational trick and the winning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "0  1.0\n",
      "1  1.0\n",
      "2  2.0\n",
      "3  1.0\n",
      "4  1.0\n"
     ]
    }
   ],
   "source": [
    "# Implementing the hashing trick in scikit-learn\n",
    "\n",
    "# Import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Get text data: text_data\n",
    "text_data = combine_text_columns(X_train)\n",
    "\n",
    "# Create the token pattern: TOKENS_ALPHANUMERIC\n",
    "TOKENS_ALPHANUMERIC = '[A-Za-z0-9]+(?=\\\\s+)' \n",
    "\n",
    "# Instantiate the HashingVectorizer: hashing_vec\n",
    "hashing_vec = HashingVectorizer(\n",
    "    norm=None,non_negative=True,token_pattern=TOKENS_ALPHANUMERIC,ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the Hashing Vectorizer\n",
    "hashed_text = hashing_vec.fit_transform(text_data)\n",
    "\n",
    "# Create DataFrame and print the head\n",
    "hashed_df = pd.DataFrame(hashed_text.data)\n",
    "print(hashed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some text is hashed to the same value, but this doesn't neccessarily hurt performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Build the winning model\n",
    "# -----------------------------------------\n",
    "\n",
    "# Import the hashing vectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Instantiate the winning model pipeline: pl\n",
    "pl = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list = [\n",
    "                ('numeric_features', Pipeline([\n",
    "                    ('selector', get_numeric_data),\n",
    "                    ('imputer', Imputer())\n",
    "                ])),\n",
    "                ('text_features', Pipeline([\n",
    "                    ('selector', get_text_data),\n",
    "                    ('vectorizer', HashingVectorizer(\n",
    "                        token_pattern=TOKENS_ALPHANUMERIC,\n",
    "                        non_negative=True, norm=None, \n",
    "                        binary=False, ngram_range=(1,2))),\n",
    "                    ('dim_red', SelectKBest(chi2, chi_k))\n",
    "                ]))\n",
    "             ]\n",
    "        )),\n",
    "        ('int', SparseInteractions(degree=2)),\n",
    "        ('scale', MaxAbsScaler()),\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Log loss score:  21.339574683\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION: USING HASHING VECTORIZER\n",
    "# ADD LOG-LOSS EVALUATION\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Fit to the training data\n",
    "pl.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print log-loss score\n",
    "y_pred = pl.predict_proba(X_test)\n",
    "score = log_loss(y_test, y_pred, eps=1e-14)\n",
    "print(\"\\nLog loss score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log loss: 1.2258. Performance is about the same, but this is expected since the HashingVectorizer should work the same as the CountVectorizer. Try this pipeline out on the whole dataset on your local machine to see its full power!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps and the social impact of your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly test ways of improving your submission\n",
    "- NLP: Stemming, stop-word removal\n",
    "- Model: RandomForest, k-NN, Nave Bayes\n",
    "- Numeric Preprocessing: Imputation strategies\n",
    "- Optimization: Grid search over pipeline objects\n",
    "- Experiment with new scikit-learn techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
